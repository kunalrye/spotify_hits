{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/joshuakowal/DSCI 303 Assignments/spotify_hits/data/tracks_w_lang.csv')\n",
    "\n",
    "#Get all songs that are in English\n",
    "df = df[df['lang'] == 'en']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "### Perform PCA on Each Decade and then do KMeans Clustering to identify groups of songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to normalize all the columns of a matrix\n",
    "\n",
    "def normalize_cols(X, m = None, s = None):\n",
    "    \"\"\"\n",
    "    Z-score normalizes all columns in matrix X\n",
    "    \n",
    "    Returns the new z_scored matrix X_n\n",
    "    \"\"\"\n",
    "    nrows, ncols = X.shape\n",
    "    X_n = np.zeros((nrows, ncols))\n",
    "    ms = []\n",
    "    ss = []\n",
    "    for i in range(ncols):\n",
    "        if m is None:\n",
    "            mean = X[:,i].mean()\n",
    "        else:\n",
    "            mean = m[i]\n",
    "        if s is None:\n",
    "            std = X[:,i].std()\n",
    "        else:\n",
    "            std = s[i]\n",
    "        X_n[:,i] = (X[:,i] - mean) / std\n",
    "        ms.append(mean)\n",
    "        ss.append(ss)\n",
    "    return X_n, ms, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.describe()\n",
    "start_dec = round(int(summary.at['min', 'year']), -1)\n",
    "last_dec = round(int(summary.at['max','year']), -1)\n",
    "\n",
    "def PCA_and_KMeans(df, start_dec, last_dec):\n",
    "    \"\"\"\n",
    "    This function takes our dataframe df and performs the following for each\n",
    "    individual decade:\n",
    "    \n",
    "    1. Perform PCA with an increasing number of principal components until we\n",
    "    get 95% of the variance covered. Once we reach this threshold, we retain\n",
    "    this transformed data in a variable called X_new. We add a tuple containing\n",
    "    the pca model and the transformed data matrix in the pcas dictionary corresponding\n",
    "    to the key representing its decade\n",
    "    \n",
    "    2. Perform K-Means Clustering with 10, 12, 14, and 16 clusters on the transformed data.\n",
    "    A dictionary containing cluster numbers corresponding to predicted labels will be added\n",
    "    to the kmeans dictionary for the given decade.\n",
    "    \n",
    "    Returns:\n",
    "    pcas - A dictionary containing {'decade':(pca_model, transformed_X)} pairs\n",
    "    kmeans - A dictionary containing {'decade':{n_clusters:pred_labels__for_n_clusters}} pairs\n",
    "    dfs - A dictionary containing {decade:[non-normalized_df, normalized_df]} pairs\n",
    "    \"\"\"\n",
    "    pcas = {}\n",
    "    kmeans = {}\n",
    "    dfs = {}\n",
    "    for dec in range(start_dec, last_dec, 10):\n",
    "        key = str(dec) + '\\'s'\n",
    "        \n",
    "        decade_df = df[df['year'] >= dec]\n",
    "        decade_df = decade_df[decade_df['year'] < (dec + 10)]\n",
    "        decade_df = decade_df.drop(labels = ['explicit', 'mode', 'year'], axis = 1)\n",
    "        \n",
    "        #Isolate a target variable, which is popularity\n",
    "        target = decade_df['popularity']\n",
    "        decade_df.drop('popularity', axis = 1)\n",
    "        dfs[key] = [decade_df]\n",
    "        X = decade_df.select_dtypes(include = 'number').to_numpy()\n",
    "        X_norm, prev_mean, prev_std = normalize_cols(X)\n",
    "        dfs[key].append(X_norm)\n",
    "        dfs[key].append(target)\n",
    "        dfs[key].append(prev_mean)\n",
    "        dfs[key].append(prev_std)\n",
    "        \n",
    "        #Do PCA until we get 95% of the variance\n",
    "        n_comp = 3\n",
    "        while True:\n",
    "            pca = PCA(n_components = n_comp)\n",
    "            X_new = pca.fit_transform(X_norm)\n",
    "            var_ret = np.sum(pca.explained_variance_ratio_)\n",
    "            if var_ret >= 0.95:\n",
    "                pcas[key] = (pca, X_new, target)\n",
    "                break\n",
    "            n_comp += 1\n",
    "        \n",
    "        #Now that we have X_new, let's do KMeans Clustering\n",
    "        #Perform clustering with 10, 12, 14, and 16 clusters\n",
    "        mods = {}\n",
    "        for i in range(10, 18, 2):\n",
    "            km = KMeans(n_clusters = i)\n",
    "            y_pred = km.fit_predict(X_new)\n",
    "            centroids = km.cluster_centers_\n",
    "            mods[i] = (X_new, y_pred, centroids)\n",
    "        kmeans[key] = mods\n",
    "    return pcas, kmeans, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dict, kmeans_dict, dataframes = PCA_and_KMeans(df, start_dec, last_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics of KMeans\n",
    "for key, val in kmeans_dict.items():\n",
    "    print('Results of KMeans for the', key)\n",
    "    for key2, val2 in val.items():\n",
    "        unique, counts = np.unique(val2[1], return_counts = True)\n",
    "        a = dict(zip(unique, counts))\n",
    "        print('For', key2, 'clusters:', a)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out the new dataframes split by decade\n",
    "for key1, value1 in dataframes.items():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in a Couple Other Datasets for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/Users/joshuakowal/DSCI 303 Assignments/spotify_hits/data/data_w_genres.csv')\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('/Users/joshuakowal/DSCI 303 Assignments/spotify_hits/data/data_by_genres.csv')\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with the Large Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 2000-2015 for the training data and 2016-2019 as the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df_numeric = df.select_dtypes(include = 'number')\n",
    "temp = df_numeric[df_numeric['year'] >= 2000]\n",
    "X = temp[temp['year'] <= 2015]\n",
    "X.drop(labels = ['explicit', 'mode', 'year'], axis = 1)\n",
    "y = X['popularity']\n",
    "X.drop('popularity', axis = 1)\n",
    "\n",
    "testX = df_numeric[df_numeric['year'] > 2015]\n",
    "testX.drop(labels = ['explicit', 'mode', 'year'], axis = 1)\n",
    "testy = testX['popularity']\n",
    "testX.drop('popularity', axis = 1)\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "testX = testX.to_numpy()\n",
    "normX, mu, sig = normalize_cols(X)\n",
    "norm_testX, mu, sig = normalize_cols(testX, mu, sig)\n",
    "\n",
    "#Make our Linear Regression model and fit with X and y\n",
    "reg = LinearRegression()\n",
    "reg.fit(normX,y)\n",
    "yhat = reg.predict(norm_testX)\n",
    "r2 = reg.score(norm_testX, testy)\n",
    "mse = np.linalg.norm(yhat - testy)\n",
    "print(\"Model score:\", r2)\n",
    "print(\"MSE with Test:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really bad model for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
